{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "import math\n",
    "import json\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "model = \"gpt-3.5-turbo-0125\"\n",
    "max_context_length = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(file_path, chunk_size):\n",
    "    file_name = os.path.basename(file_path)\n",
    "    file_name = file_name.lower()\n",
    "    if file_path:\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                text = file.read()\n",
    "        except UnicodeDecodeError:\n",
    "            with open(file_path, \"r\", encoding=\"utf-16-le\") as file:\n",
    "                text = file.read()\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "        num_tokens = len(encoding.encode(text))\n",
    "        if num_tokens > chunk_size:\n",
    "            if num_tokens > chunk_size:\n",
    "                split_size = math.ceil(num_tokens / chunk_size)\n",
    "                chunks = [\n",
    "                    text[i : i + chunk_size] for i in range(0, num_tokens, chunk_size)\n",
    "                ]\n",
    "                print(\"Text split into:\", split_size, \"chunks\")\n",
    "                return chunks, num_tokens, file_name, chunk_size\n",
    "            return text, num_tokens, file_name, chunk_size\n",
    "        else:\n",
    "            print(\"Reduce chunk size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_file(text, num_tokens):\n",
    "    if num_tokens > 4000:\n",
    "        num_tokens = 2000\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an assistant who extracts questions and the corresponding answers from texts to create a dataset for a machine learning model, you return only valid .json as well as all questions and answers in english.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f'Create as many relevant questions and the corresponding answers for this text: {text}. Return each question-answer pair in the following format: {{\"instruction\": <insert question here> , \"output\": <insert answer here>}},',\n",
    "            },\n",
    "        ],\n",
    "        temperature=0.5,\n",
    "        max_tokens=4000,\n",
    "        top_p=0.2,\n",
    "    )\n",
    "    response_ = response.choices[0].message.content\n",
    "    return response_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrickmuller/miniconda3/envs/lit-gpt/lib/python3.10/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.18) or chardet (5.2.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text split into: 4 chunks\n",
      "Total tokens in document: 3208\n",
      "chunk length: 1000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid format specifier",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m text:\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunk length:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(t))\n\u001b[0;32m---> 16\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mlabel_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_file_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     18\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(text)\n",
      "Cell \u001b[0;32mIn [4], line 14\u001b[0m, in \u001b[0;36mlabel_file\u001b[0;34m(text, num_tokens)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_tokens \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m4000\u001b[39m:\n\u001b[1;32m      3\u001b[0m     num_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2000\u001b[39m\n\u001b[1;32m      5\u001b[0m response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m      6\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      7\u001b[0m     messages\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m      8\u001b[0m         {\n\u001b[1;32m      9\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are an assistant who extracts questions and the corresponding answers from texts to create a dataset for a machine learning model, you return only valid .json as well as all questions and answers in english.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m         },\n\u001b[1;32m     12\u001b[0m         {\n\u001b[1;32m     13\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m---> 14\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCreate as many relevant questions and the corresponding answers for this text: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Return each question-answer pair in the following format: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstruction\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m <insert question here> , \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: <insert answer here>\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     15\u001b[0m         },\n\u001b[1;32m     16\u001b[0m     ],\n\u001b[1;32m     17\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m,\n\u001b[1;32m     18\u001b[0m     max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4000\u001b[39m,\n\u001b[1;32m     19\u001b[0m     top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     21\u001b[0m response_ \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response_\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid format specifier"
     ]
    }
   ],
   "source": [
    "rootDir = \"/Users/patrickmuller/Desktop/test_data\"\n",
    "file_paths = []\n",
    "output_file_name = \"V9\"\n",
    "\n",
    "for dirName, subdirList, fileList in os.walk(rootDir):\n",
    "    for fname in fileList:\n",
    "        file_paths.append(os.path.join(dirName, fname))\n",
    "\n",
    "for file in file_paths:\n",
    "    text, num_tokens, file_name, max_context_length = load_file(file, 1000)\n",
    "    print(\"Total tokens in document:\", num_tokens)\n",
    "\n",
    "    if isinstance(text, list):\n",
    "        for t in text:\n",
    "            print(\"chunk length:\", len(t))\n",
    "            text = label_file(t, num_tokens)\n",
    "            with open(output_file_name + \".jsonl\", \"a\") as f:\n",
    "                f.write(text)\n",
    "    else:\n",
    "        text = label_file(text, num_tokens)\n",
    "        with open(output_file_name + \".jsonl\", \"a\") as f:\n",
    "            f.write(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "web_scraper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
