{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfad2eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast, AutoTokenizer,AutoModelForCausalLM, AutoModel\n",
    "import evaluate\n",
    "import sys\n",
    "sys.path.insert(0, '../lit-gpt/generate')\n",
    "from pathlib import Path\n",
    "import lora\n",
    "import base\n",
    "from datasets import load_dataset\n",
    "import csv\n",
    "import random\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5ed788a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length train: 3035\n",
      "length test: 159\n",
      "length eval: 2\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data_final_split_995/test.json\") as f:\n",
    "    test_data = json.load(f)\n",
    "with open(\"../data_final_split_995/train.json\") as f:\n",
    "    train_data = json.load(f)\n",
    "with open(\"eval_dataset.json\") as f:\n",
    "    eval_data = json.load(f)\n",
    "print(\"length train: \"+str(len(train_data)))\n",
    "print(\"length test: \"+str(len(test_data)))\n",
    "eval_data = list(eval_data.items())\n",
    "print(\"length eval: \"+str(len(eval_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02e2808b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How can I reach Aalen University'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_data[0][1][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72d9fa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load('../lit-gpt/out/converted/mistralv2.pth')\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", state_dict=state_dict)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\", state_dict=state_dict)\n",
    "device = torch.device('cuda:3')  # Use GPU if available, else CPU\n",
    "def compute_perplexity(data,model):\n",
    "    model = model.to(device) \n",
    "    encodings = tokenizer(\"\\n\\n\".join(data), return_tensors=\"pt\")\n",
    "    max_length = model.config.max_position_embeddings\n",
    "    stride = 512\n",
    "    seq_len = encodings.input_ids.size(1)\n",
    "\n",
    "    nlls = []\n",
    "    prev_end_loc = 0\n",
    "    for begin_loc in tqdm(range(0, seq_len, stride)):\n",
    "        end_loc = min(begin_loc + max_length, seq_len)\n",
    "        trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
    "        input_ids = encodings.input_ids[:, begin_loc:end_loc]\n",
    "        input_ids=input_ids.to(device)\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[:, :-trg_len] = -100\n",
    "        target_ids = target_ids.to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids,labels=target_ids)\n",
    "            neg_log_likelihood = outputs.loss\n",
    "        nlls.append(neg_log_likelihood)\n",
    "        prev_end_loc = end_loc\n",
    "        if end_loc == seq_len:\n",
    "            break\n",
    "\n",
    "    ppl = torch.exp(torch.stack(nlls).mean())\n",
    "    ppl = ppl.item()\n",
    "    ppl = round(ppl,2)\n",
    "    return (ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1f1f84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_eval(data_len,data,samples,file_name):    \n",
    "    results =[]\n",
    "    n = random.sample(range(data_len-1), samples)\n",
    "    for i in range(samples-1):\n",
    "        #../lit-gpt/out/lora/mistral/lit_model_lora_finetunedV4.pth\"\n",
    "        bleu = evaluate.load(\"bleu\")\n",
    "        rouge = evaluate.load(\"rouge\")\n",
    "        model_output = lora.main(prompt = data[n[i]][0], checkpoint_dir=Path(\"../lit-gpt/checkpoints/mistralai/Mistral-7B-v0.1\"),lora_path=Path(\"../lit-gpt/out/lora/mistral/lit_model_lora_finetunedV4.pth\"))\n",
    "        #model_output = base.main(prompt = data[n[i]][0], checkpoint_dir=Path(\"../lit-gpt/out/lora/merged/v3_3k/\"))\n",
    "        bleu_result= bleu.compute(references=[data[n[i]][2]], predictions=[model_output])\n",
    "        rogue_results= rouge.compute(references=[data[n[i]][2]], predictions=[model_output])\n",
    "        ppl=compute_perplexity(data[n[i]][2],model)\n",
    "        results.append([data[n[i]][0],model_output,data[n[i]][2],ppl,round(bleu_result[\"bleu\"],2),round(rogue_results[\"rougeLsum\"],2)])\n",
    "\n",
    "    column_names = [\"Input\", \"Output\",\"Reference\", \"Perplexity\", \"BLEU\", \"Rogue\"]\n",
    "    # Write the data to a CSV file\n",
    "    with open(file_name+'.csv', 'a', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(column_names)\n",
    "        for row in results:\n",
    "            writer.writerow(row)\n",
    "\n",
    "def test_eval(data_len,data,file_name):    \n",
    "    results =[]\n",
    "    for i in range(data_len-1):\n",
    "        bleu = evaluate.load(\"bleu\")\n",
    "        rouge = evaluate.load(\"rouge\")\n",
    "        model_output = lora.main(prompt = data[i][0], checkpoint_dir=Path(\"../lit-gpt/checkpoints/mistralai/Mistral-7B-v0.1\"),lora_path=Path(\"../lit-gpt/out/lora/mistral/lit_model_lora_finetunedV4.pth\"))\n",
    "\n",
    "        bleu_result= bleu.compute(references=[data[i][2]], predictions=[model_output])\n",
    "        rogue_results= rouge.compute(references=[data[i][2]], predictions=[model_output])\n",
    "        ppl=compute_perplexity(data[i][2],model)\n",
    "        results.append([data[i][0],model_output,data[i][2],ppl,round(bleu_result[\"bleu\"],2),round(rogue_results[\"rougeLsum\"],2)])\n",
    "\n",
    "    column_names = [\"Input\", \"Output\",\"Reference\", \"Perplexity\", \"BLEU\", \"Rogue\"]\n",
    "    # Write the data to a CSV file\n",
    "    with open(file_name+'.csv', 'a', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(column_names)\n",
    "        for row in results:\n",
    "            writer.writerow(row)\n",
    "\n",
    "def eval_eval(data_len,data,file_name):\n",
    "    results =[]\n",
    "    column_names = [\"Input\", \"Output\",\"Reference\", \"Perplexity\", \"BLEU\", \"Rogue\"]\n",
    "    \n",
    "    for i in range(9,data_len-1):\n",
    "            bleu = evaluate.load(\"bleu\")\n",
    "            rouge = evaluate.load(\"rouge\")\n",
    "            print(\"...\")\n",
    "            model_output = lora.main(prompt = data[0][1][i], checkpoint_dir=Path(\"../lit-gpt/checkpoints/mistralai/Mistral-7B-v0.1\"),lora_path=Path(\"../lit-gpt/out/lora/mistral/lit_model_lora_finetunedV3.pth\"))\n",
    "            ppl=compute_perplexity(data[1][1][i],model)\n",
    "            bleu_result= bleu.compute(references=[data[1][1][i]], predictions=[model_output])\n",
    "            rogue_results= rouge.compute(references=[data[1][1][i]], predictions=[model_output])\n",
    "\n",
    "            results.append([data[0][1][i],model_output,data[1][1][i],ppl,round(bleu_result[\"bleu\"],2),round(rogue_results[\"rougeLsum\"],2)])\n",
    "\n",
    "    with open(file_name+'.csv', 'a', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(column_names)\n",
    "        for row in results:\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2c0e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eval(len(train_data),train_data,150,\"train_v3_3k\")\n",
    "#test_eval(len(test_data),test_data,\"test_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87f27457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model '../lit-gpt/checkpoints/mistralai/Mistral-7B-v0.1/lit_model.pth' with {'name': 'Mistral-7B-v0.1', 'hf_config': {'org': 'mistralai', 'name': 'Mistral-7B-v0.1'}, 'block_size': 4096, 'vocab_size': 32000, 'padding_multiple': 512, 'padded_vocab_size': 32000, 'n_layer': 32, 'n_head': 32, 'n_embd': 4096, 'rotary_percentage': 1.0, 'parallel_residual': False, 'bias': False, 'lm_head_bias': False, 'n_query_groups': 8, 'shared_attention_norm': False, '_norm_class': 'RMSNorm', 'norm_eps': 1e-05, '_mlp_class': 'LLaMAMLP', 'gelu_approximate': 'none', 'intermediate_size': 14336, 'rope_condense_ratio': 1, 'rope_base': 10000, 'n_expert': 0, 'n_expert_per_token': 0, 'r': 8, 'alpha': 16, 'dropout': 0.05, 'to_query': True, 'to_key': True, 'to_value': True, 'to_projection': False, 'to_mlp': False, 'to_head': False, 'head_size': 128, 'rope_n_elem': 128}\n",
      "Time to instantiate model: 0.07 seconds.\n",
      "Time to load the model weights: 12.73 seconds.\n",
      "Seed set to 1234\n",
      "\n",
      "\n",
      "Time for inference: 1.79 sec total, 22.84 tokens/sec\n",
      "Memory used: 14.68 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The campus has various student organizations or clubs, including the Fakultätsrat, Studienberatung, Sportwirtschaftliche Vereinigung, and Studienkolleg Konstanz.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model '../lit-gpt/checkpoints/mistralai/Mistral-7B-v0.1/lit_model.pth' with {'name': 'Mistral-7B-v0.1', 'hf_config': {'org': 'mistralai', 'name': 'Mistral-7B-v0.1'}, 'block_size': 4096, 'vocab_size': 32000, 'padding_multiple': 512, 'padded_vocab_size': 32000, 'n_layer': 32, 'n_head': 32, 'n_embd': 4096, 'rotary_percentage': 1.0, 'parallel_residual': False, 'bias': False, 'lm_head_bias': False, 'n_query_groups': 8, 'shared_attention_norm': False, '_norm_class': 'RMSNorm', 'norm_eps': 1e-05, '_mlp_class': 'LLaMAMLP', 'gelu_approximate': 'none', 'intermediate_size': 14336, 'rope_condense_ratio': 1, 'rope_base': 10000, 'n_expert': 0, 'n_expert_per_token': 0, 'r': 8, 'alpha': 16, 'dropout': 0.05, 'to_query': True, 'to_key': True, 'to_value': True, 'to_projection': False, 'to_mlp': False, 'to_head': False, 'head_size': 128, 'rope_n_elem': 128}\n",
      "Time to instantiate model: 0.06 seconds.\n",
      "Time to load the model weights: 12.74 seconds.\n",
      "Seed set to 1234\n",
      "\n",
      "\n",
      "Time for inference: 1.22 sec total, 22.90 tokens/sec\n",
      "Memory used: 14.68 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are opportunities for study abroad programs, including Erasmus, Hochschule Aalen, and Hochschule Merseburg.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model '../lit-gpt/checkpoints/mistralai/Mistral-7B-v0.1/lit_model.pth' with {'name': 'Mistral-7B-v0.1', 'hf_config': {'org': 'mistralai', 'name': 'Mistral-7B-v0.1'}, 'block_size': 4096, 'vocab_size': 32000, 'padding_multiple': 512, 'padded_vocab_size': 32000, 'n_layer': 32, 'n_head': 32, 'n_embd': 4096, 'rotary_percentage': 1.0, 'parallel_residual': False, 'bias': False, 'lm_head_bias': False, 'n_query_groups': 8, 'shared_attention_norm': False, '_norm_class': 'RMSNorm', 'norm_eps': 1e-05, '_mlp_class': 'LLaMAMLP', 'gelu_approximate': 'none', 'intermediate_size': 14336, 'rope_condense_ratio': 1, 'rope_base': 10000, 'n_expert': 0, 'n_expert_per_token': 0, 'r': 8, 'alpha': 16, 'dropout': 0.05, 'to_query': True, 'to_key': True, 'to_value': True, 'to_projection': False, 'to_mlp': False, 'to_head': False, 'head_size': 128, 'rope_n_elem': 128}\n",
      "Time to instantiate model: 0.07 seconds.\n",
      "Time to load the model weights: 12.68 seconds.\n",
      "Seed set to 1234\n",
      "\n",
      "\n",
      "Time for inference: 1.13 sec total, 22.97 tokens/sec\n",
      "Memory used: 14.68 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The university offers majors in Computer Science, Business Informatics, Informatik, and Medieninformatik.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model '../lit-gpt/checkpoints/mistralai/Mistral-7B-v0.1/lit_model.pth' with {'name': 'Mistral-7B-v0.1', 'hf_config': {'org': 'mistralai', 'name': 'Mistral-7B-v0.1'}, 'block_size': 4096, 'vocab_size': 32000, 'padding_multiple': 512, 'padded_vocab_size': 32000, 'n_layer': 32, 'n_head': 32, 'n_embd': 4096, 'rotary_percentage': 1.0, 'parallel_residual': False, 'bias': False, 'lm_head_bias': False, 'n_query_groups': 8, 'shared_attention_norm': False, '_norm_class': 'RMSNorm', 'norm_eps': 1e-05, '_mlp_class': 'LLaMAMLP', 'gelu_approximate': 'none', 'intermediate_size': 14336, 'rope_condense_ratio': 1, 'rope_base': 10000, 'n_expert': 0, 'n_expert_per_token': 0, 'r': 8, 'alpha': 16, 'dropout': 0.05, 'to_query': True, 'to_key': True, 'to_value': True, 'to_projection': False, 'to_mlp': False, 'to_head': False, 'head_size': 128, 'rope_n_elem': 128}\n",
      "Time to instantiate model: 0.06 seconds.\n",
      "Time to load the model weights: 12.69 seconds.\n",
      "Seed set to 1234\n",
      "\n",
      "\n",
      "Time for inference: 1.35 sec total, 22.99 tokens/sec\n",
      "Memory used: 14.68 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sie können sich an den Leiter des Studiengangs oder die Prüfungsausschussvorsitzende wenden.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model '../lit-gpt/checkpoints/mistralai/Mistral-7B-v0.1/lit_model.pth' with {'name': 'Mistral-7B-v0.1', 'hf_config': {'org': 'mistralai', 'name': 'Mistral-7B-v0.1'}, 'block_size': 4096, 'vocab_size': 32000, 'padding_multiple': 512, 'padded_vocab_size': 32000, 'n_layer': 32, 'n_head': 32, 'n_embd': 4096, 'rotary_percentage': 1.0, 'parallel_residual': False, 'bias': False, 'lm_head_bias': False, 'n_query_groups': 8, 'shared_attention_norm': False, '_norm_class': 'RMSNorm', 'norm_eps': 1e-05, '_mlp_class': 'LLaMAMLP', 'gelu_approximate': 'none', 'intermediate_size': 14336, 'rope_condense_ratio': 1, 'rope_base': 10000, 'n_expert': 0, 'n_expert_per_token': 0, 'r': 8, 'alpha': 16, 'dropout': 0.05, 'to_query': True, 'to_key': True, 'to_value': True, 'to_projection': False, 'to_mlp': False, 'to_head': False, 'head_size': 128, 'rope_n_elem': 128}\n",
      "Time to instantiate model: 0.06 seconds.\n",
      "Time to load the model weights: 12.80 seconds.\n",
      "Seed set to 1234\n",
      "\n",
      "\n",
      "Time for inference: 4.33 sec total, 23.10 tokens/sec\n",
      "Memory used: 14.68 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die Zulassungsvoraussetzungen für die Universität waren: Abitur oder Fachhochschulreife, gute schulische Leistungen, Interesse an Informatik und Technik, Bewerber mit MINT-Begleitern, Bewerber mit MINT-Begleitern, Frauen, Schüler mit guten schulischen Leistungen, und Bewerber aus sozial schwierigen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model '../lit-gpt/checkpoints/mistralai/Mistral-7B-v0.1/lit_model.pth' with {'name': 'Mistral-7B-v0.1', 'hf_config': {'org': 'mistralai', 'name': 'Mistral-7B-v0.1'}, 'block_size': 4096, 'vocab_size': 32000, 'padding_multiple': 512, 'padded_vocab_size': 32000, 'n_layer': 32, 'n_head': 32, 'n_embd': 4096, 'rotary_percentage': 1.0, 'parallel_residual': False, 'bias': False, 'lm_head_bias': False, 'n_query_groups': 8, 'shared_attention_norm': False, '_norm_class': 'RMSNorm', 'norm_eps': 1e-05, '_mlp_class': 'LLaMAMLP', 'gelu_approximate': 'none', 'intermediate_size': 14336, 'rope_condense_ratio': 1, 'rope_base': 10000, 'n_expert': 0, 'n_expert_per_token': 0, 'r': 8, 'alpha': 16, 'dropout': 0.05, 'to_query': True, 'to_key': True, 'to_value': True, 'to_projection': False, 'to_mlp': False, 'to_head': False, 'head_size': 128, 'rope_n_elem': 128}\n",
      "Time to instantiate model: 0.06 seconds.\n",
      "Time to load the model weights: 12.85 seconds.\n",
      "Seed set to 1234\n",
      "\n",
      "\n",
      "Time for inference: 0.79 sec total, 22.91 tokens/sec\n",
      "Memory used: 14.69 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die Studiengebühr beträgt 186 Euro pro Semester.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "eval_eval(16,eval_data,\"eval_v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cebe3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "       Perplexity        BLEU       Rogue\n",
      "count  299.000000  299.000000  299.000000\n",
      "mean     2.213110    0.623170    0.746629\n",
      "std      0.380156    0.349394    0.269124\n",
      "min      1.630000    0.000000    0.050000\n",
      "25%      1.970000    0.341808    0.559900\n",
      "50%      2.100000    0.678369    0.846154\n",
      "75%      2.310000    1.000000    1.000000\n",
      "max      3.960000    1.000000    1.000000\n",
      "\n",
      "Test\n",
      "       Perplexity        BLEU       Rogue\n",
      "count  158.000000  158.000000  158.000000\n",
      "mean     2.154367    0.539843    0.700639\n",
      "std      0.326294    0.342981    0.263484\n",
      "min      1.630000    0.000000    0.153846\n",
      "25%      1.960000    0.240998    0.500000\n",
      "50%      2.090000    0.515790    0.750927\n",
      "75%      2.260000    0.862545    0.963268\n",
      "max      3.450000    1.000000    1.000000\n",
      "\n",
      "Eval\n",
      "       Perplexity\n",
      "count   49.000000\n",
      "mean     2.827551\n",
      "std      0.656460\n",
      "min      1.980000\n",
      "25%      2.240000\n",
      "50%      2.860000\n",
      "75%      3.310000\n",
      "max      4.450000\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV data into a pandas DataFrame\n",
    "test = pd.read_csv('test_v1_6k.csv')\n",
    "train = pd.read_csv('train_v1_6k.csv')\n",
    "eval = pd.read_csv('eval_.csv')\n",
    "\n",
    "# Compute and print other statistical parameters\n",
    "print(\"Train\")\n",
    "print(train.describe())\n",
    "print(\"\")\n",
    "print(\"Test\")\n",
    "print(test.describe())\n",
    "print(\"\")\n",
    "print(\"Eval\")\n",
    "print(eval.describe())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
